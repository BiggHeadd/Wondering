[TOC]

# 正则化

> loss function的基础上，加上了一些正则化项或者称为模型复杂度惩罚项。

- L1正则化&&L2正则化

- 以线性回归的Loss为例：

$$
L=\frac{1}{N}*\sum\nolimits_{i=1}^{N}(y_i-\omega^Tx_i)^2
$$



## L1正则化

- L1正则化项(lasso回归)

$$
L1=\frac{1}{N}*\sum\nolimits_{i=1}^{N}(C||\omega||_1)
$$



## L2正则化

- L2正则化项目(岭回归)

$$
L2=\frac{1}{N}*\sum\nolimits_{i=1}^{N}(C||\omega||^2_2)
$$



## 结构风险最小化

> 在经验风险最小化的基础上，也即是训练误差最小化，尽可能采用简单的模型，以此提高泛化预测精准度

- 降低拟合程度
  - 给loss function 加上正则化项，新的优化目标函数变为：h=f+normal，这时候在优化过程中就会需要在f和normal中间进行权衡（trade-off），如果还像原来只优化f，那可能得到一组解比较复杂，使得正则化项normal比较大，那么此时h就不是最优解，因此可以看出，正则化项能使求到的解更简单，符合奥卡姆剃刀原理，同时也比较符合在偏差和方差（方差表示模型复杂度）分析中，通过降低模型复杂度，得到更小的泛化误差，降低过拟合程度。
- L1正则化、L2正则化
  - L1正则化就是在loss function后面加上L1范数，加上L1范数容易得到稀疏解（0比较多）。
  - L2正则化就是在loss function后面加上L2范数的平方，加上L2正则化相比于L1正则化来说，得到的解比较平滑（没那么稀疏），但是同样能够保证解中接近于0（但不是0，所以相对平滑）的维度更多，从而降低模型复杂度。



## 奥卡姆剃刀原理

- 大自然不做任何多余的事。如果你有两个原理，它们都能解释观测到的事实，那么你应该使用简单的那个，直到发现更多的证据。对于现象最简单的解释往往比复杂的解释更正确。如果你有两个类似的解决方案，选择最简单的、需要最少假设的解释最有可能是正确的。一句话：把烦琐累赘一刀砍掉，让事情保持简单！